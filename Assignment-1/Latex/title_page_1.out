\BOOKMARK [1][-]{section.1}{Softmax [2 points]}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{ [1 point] Prove that softmax is invariant to constant sifts in the input, i.e., for any input vector x and a constant scalar c, the following holds: \040 softmax\(x\) = softmax\(x +c\) \040where \040softmax\(x\)i \040-1 exii' exi', and \040x +c \040means adding c to every dimension of x.}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{ [1 point] Let z = W x + c, where W and c are some matrix and vector, respectively. Let \040 J = ilog \040softmax\(zi\) \040Calculate the derivatives of J w.r.t. W and c, respectively, i.e., calculate \040JW \040and \040Jc .}{section.1}% 3
\BOOKMARK [1][-]{section.2}{Logistic Regression with Regularization [2 points]}{}% 4
\BOOKMARK [2][-]{subsection.2.1}{ [1 point] Let the data be \(xi, yi\)i=1N \040where \040xi Rd \040and \040yi \1730,1\175 . Logistic regression is a binary classification model, with the probability of yi being 1 as: \040p\(yi;xi,\) = \(Txi\) \040-1 11 + e-Txi \040where \040is the model parameter. Assume we impose an L2 regularization term on the parameter, defined as: \040R\(\) = \040-1 2T with a positive constant \040. Write out the final objective function for this logistic regression with regularization model. }{section.2}% 5
\BOOKMARK [2][-]{subsection.2.2}{[1 point] If we use gradient descent to solve the model parameter. Derive the updating rule for \040. Your answer should contain the derivation, not just the final answer}{section.2}% 6
\BOOKMARK [1][-]{section.3}{Derivative of the Softmax Function [3 points]}{}% 7
\BOOKMARK [2][-]{subsection.3.1}{ [1 point] Define the loss function as \040J\(z\) = -k=1Kyk \040log y"0365yk \040where y"0365yk = \040-1 ezkk' ezk' , and \(y1,.., yK\) is a known probability vector. Derive the \040J\(z\)z . Note z = \(z1,.., zK\) is a vector so \040J\(z\)z \040is in the form of a vector. Your answer should contain the derivation, not just the final answer.}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.2}{ [1 point] Assume the above softmax is the output layer of an FNN. Briefly explain how the derivative is used in the back propagation algorithm.}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.3}{ [1 points] Let z = WT h + b, where W is a matrix, b and h are vectors. Use the chain rule to calculate the gradient of W and b, i.e., \040JW \040and \040Jb , respectively}{section.3}% 10
\BOOKMARK [1][-]{section.4}{MNIST with FNN [3 points]}{}% 11
\BOOKMARK [2][-]{subsection.4.1}{[3 points] Design an FNN for MNIST classification. Implement the model and plot two curves in one figure: i\) training loss vs. training iterations; ii\) test loss vs. training iterations. \205 You can use code from websites. However, you must reference \(cite\) the code in your answer. \205 Submission includes the plot of the two curves and the runnable code \(with a ReadMe file containing instructions on how to run the code\).}{section.4}% 12
