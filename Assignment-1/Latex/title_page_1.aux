\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {Q1}Softmax [2 points]}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(1)} [1 point] Prove that softmax is invariant to constant sifts in the input, i.e., for any input vector x and a constant scalar c, the following holds: \\ $$ softmax(x) = softmax(x +c) $$ where $ softmax(x)_i \triangleq \genfrac  {}{}{}0{e^{x_i}}{\DOTSB \sum@ \slimits@ _{i_{'}}^{} e^{x_i^{'}}},$ and $ x +c $ means adding c to every dimension of x.}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(2)} [1 point] Let $z = W x + c$, where W and c are some matrix and vector, respectively. Let \\ $$ J = \DOTSB \sum@ \slimits@ _{i}^{}log \tmspace  +\thinmuskip {.1667em} softmax(z_{i}) $$ Calculate the derivatives of J w.r.t. W and c, respectively, i.e., calculate $ \frac  {\partial J}{\partial W} $ and $ \frac  {\partial J}{\partial c} $.}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {Q2}Logistic Regression with Regularization [2 points]}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(1)} [1 point] Let the data be $(x_i, y_i)_{i=1}^{N} $ where $ x_i \in R^{d} $ and $ y_i \in \{0,1\} $. Logistic regression is a binary classification model, with the probability of $y_i$ being 1 as: $$ p(y_i;x_i,\theta ) = \sigma (\theta ^Tx_i) \triangleq \genfrac  {}{}{}0{1}{1 + e^{-\theta ^Tx_{i}}} $$ where $\theta $ is the model parameter. Assume we impose an L2 regularization term on the parameter, defined as: $$ R(\theta ) = \genfrac  {}{}{}0{\lambda }{2}\theta ^T\theta $$ with a positive constant $ \lambda $. Write out the final objective function for this logistic regression with regularization model. }{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(2)}[1 point] If we use gradient descent to solve the model parameter. Derive the updating rule for $\theta $ . Your answer should contain the derivation, not just the final answer}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {Q3}Derivative of the Softmax Function [3 points]}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(1)} [1 point] Define the loss function as $$ J(z) = -\DOTSB \sum@ \slimits@ _{k=1}^{K}y_k \tmspace  +\thinmuskip {.1667em} log \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle y$}\mathaccent "0365{y}_k $$ where $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle y$}\mathaccent "0365{y}_k = \genfrac  {}{}{}0{e^{z_k}}{\DOTSB \sum@ \slimits@ _{k^{'}}^{} e^{z_{k^{'}}}}$ , and $(y_1,.., y_K)$ is a known probability vector. Derive the $ \frac  {\partial J(z)}{\partial z} $. Note $z = (z_1,.., z_K)$ is a vector so $ \frac  {\partial J(z)}{\partial z} $ is in the form of a vector. Your answer should contain the derivation, not just the final answer.}{5}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(2)} [1 point] Assume the above softmax is the output layer of an FNN. Briefly explain how the derivative is used in the back propagation algorithm.}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(3)} [1 points] Let $z = W^T h + b$, where W is a matrix, b and h are vectors. Use the chain rule to calculate the gradient of W and b, i.e., $ \frac  {\partial J}{\partial W} $ and $ \frac  {\partial J}{\partial b} $, respectively}{6}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {Q4}MNIST with FNN [3 points]}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(1)}[3 points] Design an FNN for MNIST classification. Implement the model and plot two curves in one figure: i) training loss vs. training iterations; ii) test loss vs. training iterations.\\ \IeC {\textendash } You can use code from websites. However, you must reference (cite) the code in your answer.\\ \IeC {\textendash } Submission includes the plot of the two curves and the runnable code (with a ReadMe file containing instructions on how to run the code).}{6}{subsection.4.1}}
